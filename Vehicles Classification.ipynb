{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc6eb0ae-1364-4e6a-80b4-0e3a31d3d06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\muskan\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\muskan\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-3.0.2-py3-none-win_amd64.whl (150.0 MB)\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 11.5/150.0 MB 72.3 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 28.8/150.0 MB 79.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 38.3/150.0 MB 78.5 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 51.4/150.0 MB 68.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 66.6/150.0 MB 68.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 81.3/150.0 MB 69.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 98.6/150.0 MB 72.3 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 117.4/150.0 MB 74.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 137.6/150.0 MB 77.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.9/150.0 MB 78.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.9/150.0 MB 78.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.9/150.0 MB 78.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 150.0/150.0 MB 61.4 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb202b92-5581-44c5-baea-fd0167f405ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap\n",
      "  Downloading shap-0.48.0-cp312-cp312-win_amd64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\muskan\\anaconda3\\lib\\site-packages (from shap) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\muskan\\anaconda3\\lib\\site-packages (from shap) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\muskan\\anaconda3\\lib\\site-packages (from shap) (1.5.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\muskan\\anaconda3\\lib\\site-packages (from shap) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\muskan\\anaconda3\\lib\\site-packages (from shap) (4.66.5)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\muskan\\anaconda3\\lib\\site-packages (from shap) (24.1)\n",
      "Collecting slicer==0.0.8 (from shap)\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: numba>=0.54 in c:\\users\\muskan\\anaconda3\\lib\\site-packages (from shap) (0.60.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\muskan\\anaconda3\\lib\\site-packages (from shap) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\muskan\\anaconda3\\lib\\site-packages (from shap) (4.11.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\muskan\\anaconda3\\lib\\site-packages (from numba>=0.54->shap) (0.43.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\muskan\\anaconda3\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\muskan\\anaconda3\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\muskan\\anaconda3\\lib\\site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\muskan\\anaconda3\\lib\\site-packages (from pandas->shap) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\muskan\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\muskan\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\muskan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Downloading shap-0.48.0-cp312-cp312-win_amd64.whl (545 kB)\n",
      "   ---------------------------------------- 0.0/545.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 545.3/545.3 kB 9.1 MB/s eta 0:00:00\n",
      "Downloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: slicer, shap\n",
      "Successfully installed shap-0.48.0 slicer-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80bf1d9-049a-467a-9243-2c9eb3a8ddec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting Enhanced Classification Analysis with Comprehensive Model Comparison...\n",
      "====================================================================================================\n",
      "COMPREHENSIVE CLASSIFICATION ANALYSIS - ENHANCED MODEL COMPARISON\n",
      "====================================================================================================\n",
      " Loaded dataset: (304798, 26)\n",
      "\n",
      "================================================================================\n",
      "CLASSIFICATION MODELING PIPELINE - ENHANCED MODEL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Creating price categories for classification...\n",
      "Price category distribution:\n",
      "price_category\n",
      "Premium      2486\n",
      "Mid-Range    1718\n",
      "Budget       1182\n",
      "Luxury        379\n",
      "Name: count, dtype: int64\n",
      "Percentage distribution:\n",
      "price_category\n",
      "Premium      43.122290\n",
      "Mid-Range    29.800520\n",
      "Budget       20.503036\n",
      "Luxury        6.574154\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE FEATURE ENGINEERING - CLASSIFICATION MODELS\n",
      "================================================================================\n",
      "Converting categorical columns to object type...\n",
      "Starting features: 27\n",
      "Created 10 temporal features\n",
      "Created 10 odometer-based features\n",
      "Created 6 manufacturer-based features\n",
      "\n",
      "Final feature count: 53\n",
      "\n",
      "Modeling dataset shape: (5765, 53)\n",
      "Target classes: ['Budget' 'Luxury' 'Mid-Range' 'Premium']\n",
      "Number of classes: 4\n",
      "\n",
      "Training models with 25 features...\n",
      "\n",
      " Training DecisionTree...\n",
      "  Test Accuracy: 0.595\n",
      "  AUC Score: 0.827\n",
      "  F1-Score: 0.603\n",
      "\n",
      " Training RandomForest...\n",
      "  Test Accuracy: 0.650\n",
      "  AUC Score: 0.863\n",
      "  F1-Score: 0.653\n",
      "\n",
      " Training XGBoost...\n",
      "  Test Accuracy: 0.729\n",
      "  AUC Score: 0.901\n",
      "  F1-Score: 0.718\n",
      "\n",
      " Training LogisticRegression...\n",
      "  Test Accuracy: 0.645\n",
      "  AUC Score: 0.831\n",
      "  F1-Score: 0.627\n",
      "\n",
      " Training SVM...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_curve, auc,\n",
    "                           precision_recall_curve, accuracy_score, precision_score,\n",
    "                           recall_score, f1_score, roc_auc_score, average_precision_score)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import warnings\n",
    "import shap\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def safe_categorical_handling(df):\n",
    "    \"\"\"Safely convert all categorical columns to object type to avoid categorical errors\"\"\"\n",
    "    print(\"Converting categorical columns to object type...\")\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype.name == 'category':\n",
    "            df[col] = df[col].astype('object')\n",
    "    return df\n",
    "\n",
    "def create_price_categories(df, price_col='price'):\n",
    "    \"\"\"Create price categories for classification\"\"\"\n",
    "    print(\"\\nCreating price categories for classification...\")\n",
    "\n",
    "    # Remove rows with missing or invalid prices\n",
    "    df_clean = df[df[price_col].notna()].copy()\n",
    "    df_clean = df_clean[(df_clean[price_col] > 0) & (df_clean[price_col] <= 200000)]\n",
    "\n",
    "    # Create price categories based on realistic vehicle price ranges\n",
    "    df_clean['price_category'] = pd.cut(df_clean[price_col],\n",
    "                                      bins=[0, 8000, 20000, 40000, np.inf],\n",
    "                                      labels=['Budget', 'Mid-Range', 'Premium', 'Luxury'])\n",
    "\n",
    "    # Display distribution\n",
    "    print(\"Price category distribution:\")\n",
    "    print(df_clean['price_category'].value_counts())\n",
    "    print(f\"Percentage distribution:\")\n",
    "    print(df_clean['price_category'].value_counts(normalize=True) * 100)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "def comprehensive_feature_engineering_classification(df, target_col='price_category'):\n",
    "    \"\"\"COMPREHENSIVE FEATURE ENGINEERING FOR CLASSIFICATION\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE FEATURE ENGINEERING - CLASSIFICATION MODELS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # First, handle any existing categorical columns\n",
    "    df_features = safe_categorical_handling(df.copy())\n",
    "\n",
    "    # Clean and convert basic columns\n",
    "    for col in ['year', 'odometer', 'price']:\n",
    "        if col in df_features.columns:\n",
    "            df_features[col] = pd.to_numeric(df_features[col], errors='coerce')\n",
    "\n",
    "    print(f\"Starting features: {df_features.shape[1]}\")\n",
    "\n",
    "    # 1. TEMPORAL FEATURES (from year)\n",
    "    if 'year' in df_features.columns:\n",
    "        current_year = 2021\n",
    "        df_features['vehicle_age'] = current_year - df_features['year']\n",
    "        df_features['age_squared'] = df_features['vehicle_age'] ** 2\n",
    "        df_features['is_new'] = (df_features['vehicle_age'] <= 3).astype(int)\n",
    "        df_features['is_old'] = (df_features['vehicle_age'] >= 15).astype(int)\n",
    "        df_features['is_vintage'] = (df_features['vehicle_age'] >= 25).astype(int)\n",
    "\n",
    "        # Safe age categorization with numeric encoding\n",
    "        def safe_categorize_age(age):\n",
    "            if pd.isna(age):\n",
    "                return 0\n",
    "            elif age <= 3:\n",
    "                return 1\n",
    "            elif age <= 7:\n",
    "                return 2\n",
    "            elif age <= 15:\n",
    "                return 3\n",
    "            elif age <= 25:\n",
    "                return 4\n",
    "            else:\n",
    "                return 5\n",
    "\n",
    "        df_features['age_category_numeric'] = df_features['vehicle_age'].apply(safe_categorize_age)\n",
    "        df_features['decade'] = (df_features['year'] // 10) * 10\n",
    "        df_features['is_2010s'] = ((df_features['year'] >= 2010) & (df_features['year'] <= 2019)).astype(int)\n",
    "        df_features['is_2000s'] = ((df_features['year'] >= 2000) & (df_features['year'] <= 2009)).astype(int)\n",
    "        print(\"Created 10 temporal features\")\n",
    "\n",
    "    # 2. ODOMETER-BASED FEATURES\n",
    "    if 'odometer' in df_features.columns:\n",
    "        df_features['log_odometer'] = np.log1p(df_features['odometer'].fillna(0))\n",
    "        df_features['sqrt_odometer'] = np.sqrt(df_features['odometer'].fillna(0))\n",
    "\n",
    "        def safe_categorize_mileage(mileage):\n",
    "            if pd.isna(mileage):\n",
    "                return 0\n",
    "            elif mileage <= 30000:\n",
    "                return 1\n",
    "            elif mileage <= 60000:\n",
    "                return 2\n",
    "            elif mileage <= 100000:\n",
    "                return 3\n",
    "            elif mileage <= 150000:\n",
    "                return 4\n",
    "            else:\n",
    "                return 5\n",
    "\n",
    "        df_features['mileage_category_numeric'] = df_features['odometer'].apply(safe_categorize_mileage)\n",
    "        df_features['low_mileage'] = (df_features['odometer'] <= 50000).fillna(False).astype(int)\n",
    "        df_features['high_mileage'] = (df_features['odometer'] >= 100000).fillna(False).astype(int)\n",
    "        df_features['very_high_mileage'] = (df_features['odometer'] >= 150000).fillna(False).astype(int)\n",
    "\n",
    "        if 'vehicle_age' in df_features.columns:\n",
    "            df_features['mileage_per_year'] = df_features['odometer'] / (df_features['vehicle_age'] + 1)\n",
    "            df_features['low_mileage_for_age'] = (df_features['mileage_per_year'] < 10000).fillna(False).astype(int)\n",
    "            df_features['high_mileage_for_age'] = (df_features['mileage_per_year'] > 15000).fillna(False).astype(int)\n",
    "            df_features['mileage_age_interaction'] = df_features['odometer'] * df_features['vehicle_age']\n",
    "        print(\"Created 10 odometer-based features\")\n",
    "\n",
    "    # 3. MANUFACTURER-BASED FEATURES\n",
    "    if 'manufacturer' in df_features.columns:\n",
    "        df_features['manufacturer'] = df_features['manufacturer'].fillna('unknown').astype(str)\n",
    "        df_features['manufacturer_clean'] = df_features['manufacturer'].str.lower().str.strip()\n",
    "\n",
    "        luxury_brands = ['bmw', 'mercedes-benz', 'audi', 'lexus', 'acura', 'infiniti',\n",
    "                        'cadillac', 'lincoln', 'volvo', 'jaguar', 'porsche', 'tesla']\n",
    "        reliable_brands = ['toyota', 'honda', 'nissan', 'mazda', 'subaru', 'hyundai', 'kia']\n",
    "        american_brands = ['ford', 'chevrolet', 'gmc', 'dodge', 'jeep', 'chrysler', 'buick', 'pontiac']\n",
    "        european_brands = ['bmw', 'mercedes-benz', 'audi', 'volkswagen', 'volvo', 'jaguar',\n",
    "                          'land rover', 'mini', 'porsche', 'ferrari', 'alfa-romeo']\n",
    "\n",
    "        df_features['is_luxury'] = df_features['manufacturer_clean'].isin(luxury_brands).astype(int)\n",
    "        df_features['is_reliable'] = df_features['manufacturer_clean'].isin(reliable_brands).astype(int)\n",
    "        df_features['is_american'] = df_features['manufacturer_clean'].isin(american_brands).astype(int)\n",
    "        df_features['is_european'] = df_features['manufacturer_clean'].isin(european_brands).astype(int)\n",
    "        df_features['is_japanese'] = df_features['manufacturer_clean'].isin(['toyota', 'honda', 'nissan', 'mazda', 'subaru', 'lexus', 'acura', 'infiniti']).astype(int)\n",
    "\n",
    "        def safe_categorize_brand(manufacturer):\n",
    "            if manufacturer in luxury_brands:\n",
    "                return 3\n",
    "            elif manufacturer in reliable_brands:\n",
    "                return 2\n",
    "            else:\n",
    "                return 1\n",
    "\n",
    "        df_features['brand_tier_numeric'] = df_features['manufacturer_clean'].apply(safe_categorize_brand)\n",
    "        print(\"Created 6 manufacturer-based features\")\n",
    "\n",
    "    # Add remaining feature engineering sections (condition, fuel, transmission, etc.)\n",
    "    # [Previous feature engineering code continues here...]\n",
    "\n",
    "    print(f\"\\nFinal feature count: {df_features.shape[1]}\")\n",
    "    return df_features\n",
    "\n",
    "def classification_modeling_pipeline(df, target_col='price_category'):\n",
    "    \"\"\"CLASSIFICATION MODELING PIPELINE with enhanced comparison metrics\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CLASSIFICATION MODELING PIPELINE - ENHANCED MODEL COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 1. Create price categories and feature engineering\n",
    "    df_with_categories = create_price_categories(df)\n",
    "    df_features = comprehensive_feature_engineering_classification(df_with_categories, target_col)\n",
    "    df_model = df_features[df_features[target_col].notna()].copy()\n",
    "\n",
    "    print(f\"\\nModeling dataset shape: {df_model.shape}\")\n",
    "\n",
    "    # 2. Prepare features and target\n",
    "    exclude_cols = [target_col, 'price', 'log_price', 'price_category']\n",
    "    text_columns = df_model.select_dtypes(include=['object']).columns.tolist()\n",
    "    exclude_cols.extend(text_columns)\n",
    "    feature_cols = [col for col in df_model.columns if col not in exclude_cols]\n",
    "\n",
    "    X = df_model[feature_cols].copy()\n",
    "    y = df_model[target_col].copy()\n",
    "\n",
    "    # 3. Data preprocessing\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "    X = X.select_dtypes(include=[np.number])\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    all_nan_cols = X.columns[X.isnull().all()].tolist()\n",
    "    if all_nan_cols:\n",
    "        X = X.drop(columns=all_nan_cols)\n",
    "\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    X = pd.DataFrame(X_imputed, columns=X.columns, index=X.index)\n",
    "\n",
    "    # 4. Feature selection\n",
    "    variances = X.var()\n",
    "    high_variance_features = variances[variances > 0.01].index.tolist()\n",
    "\n",
    "    if high_variance_features:\n",
    "        X = X[high_variance_features]\n",
    "\n",
    "    max_features = min(25, X.shape[1])\n",
    "    if max_features > 0:\n",
    "        try:\n",
    "            selector = SelectKBest(score_func=f_classif, k=max_features)\n",
    "            X_selected = selector.fit_transform(X, y)\n",
    "            selected_features = X.columns[selector.get_support()].tolist()\n",
    "            X_final = pd.DataFrame(X_selected, columns=selected_features, index=X.index)\n",
    "        except:\n",
    "            X_final = X\n",
    "    else:\n",
    "        X_final = X\n",
    "\n",
    "    # 5. Encode target variable\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    class_names = le.classes_\n",
    "    n_classes = len(class_names)\n",
    "\n",
    "    print(f\"Target classes: {class_names}\")\n",
    "    print(f\"Number of classes: {n_classes}\")\n",
    "\n",
    "    # 6. Split data\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X_final, y_encoded,\n",
    "        test_size=0.20,\n",
    "        random_state=42,\n",
    "        stratify=y_encoded\n",
    "    )\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=0.25,               # 0.25 * 80% = 20% of original\n",
    "        random_state=42,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "\n",
    "    # 7. Scale features\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    # 8. Define models\n",
    "    models = {\n",
    "        'DecisionTree': DecisionTreeClassifier(\n",
    "            max_depth=10,\n",
    "            min_samples_split=20,\n",
    "            min_samples_leaf=10,\n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        ),\n",
    "        'RandomForest': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=20,\n",
    "            min_samples_leaf=10,\n",
    "            max_features='sqrt',\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'XGBoost': xgb.XGBClassifier(\n",
    "            objective='multi:softmax',\n",
    "            num_class=n_classes,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "\n",
    "            # Complexity control\n",
    "            max_depth=4,                # simpler trees to reduce variance [1]\n",
    "            min_child_weight=7,         # larger leaf weight threshold [1]\n",
    "\n",
    "            # Randomness\n",
    "            subsample=0.7,              # fewer rows per tree [1]\n",
    "            colsample_bytree=0.6,       # fewer features per tree [1]\n",
    "\n",
    "            # Regularization\n",
    "            reg_alpha=0.1,              # stronger L1 penalty [6]\n",
    "            reg_lambda=1.0,             # stronger L2 penalty [6]\n",
    "            gamma=0.5,                  # higher split gain threshold [1]\n",
    "\n",
    "            # Learning rate and rounds\n",
    "            learning_rate=0.05,         # smaller steps for generalization [3]\n",
    "            n_estimators=500,           # more trees to compensate [3]\n",
    "\n",
    "            # Early stopping\n",
    "            #early_stopping_rounds=30,    # stop when no improvement on validation [2]\n",
    "            #eval_metric='mlogloss'\n",
    "        ),\n",
    "    'LogisticRegression': LogisticRegression(\n",
    "        penalty='l2',\n",
    "        C=1.0,\n",
    "        solver='lbfgs',\n",
    "        multi_class='auto',\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'SVM': SVC(\n",
    "        C=1.0,\n",
    "        kernel='rbf',\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    }\n",
    "\n",
    "    # 9. Train models and collect comprehensive metrics\n",
    "    results = {}\n",
    "    model_objects = {}\n",
    "    predictions = {}\n",
    "\n",
    "    print(f\"\\nTraining models with {X_final.shape[1]} features...\")\n",
    "\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            print(f\"\\n Training {name}...\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Use appropriate data scaling\n",
    "            if name in ['LogisticRegression']:\n",
    "                X_train_model, X_test_model = X_train_scaled, X_test_scaled\n",
    "            else:\n",
    "                X_train_model, X_test_model = X_train.values, X_test.values\n",
    "\n",
    "            # Train model\n",
    "            model.fit(X_train_model, y_train)\n",
    "\n",
    "\n",
    "            # Predictions\n",
    "            y_pred_train = model.predict(X_train_model)\n",
    "            y_pred_test = model.predict(X_test_model)\n",
    "            y_pred_proba = model.predict_proba(X_test_model)\n",
    "\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            # Basic metrics\n",
    "            train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "            test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "            precision = precision_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
    "            recall = recall_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
    "\n",
    "            # AUC scores\n",
    "            try:\n",
    "                if n_classes == 2:\n",
    "                    auc_score = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "                    avg_precision = average_precision_score(y_test, y_pred_proba[:, 1])\n",
    "                else:\n",
    "                    auc_score = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "                    avg_precision = average_precision_score(\n",
    "                        label_binarize(y_test, classes=range(n_classes)),\n",
    "                        y_pred_proba, average='weighted'\n",
    "                    )\n",
    "            except:\n",
    "                auc_score = 0.0\n",
    "                avg_precision = 0.0\n",
    "\n",
    "            # Cross-validation\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            cv_scores = cross_val_score(model, X_train_model, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "            results[name] = {\n",
    "                'CV_Accuracy_Mean': cv_scores.mean(),\n",
    "                'CV_Accuracy_Std': cv_scores.std(),\n",
    "                'Train_Accuracy': train_accuracy,\n",
    "                'Test_Accuracy': test_accuracy,\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F1_Score': f1,\n",
    "                'AUC_Score': auc_score,\n",
    "                'Avg_Precision': avg_precision,\n",
    "                'Overfitting_Gap': train_accuracy - test_accuracy,\n",
    "                'Training_Time': training_time,\n",
    "                'Features_Used': X_final.shape[1]\n",
    "            }\n",
    "\n",
    "            # Store for visualizations\n",
    "            model_objects[name] = model\n",
    "            predictions[name] = {\n",
    "                'y_pred': y_pred_test,\n",
    "                'y_pred_proba': y_pred_proba,\n",
    "                'y_train': y_train,\n",
    "                'y_test': y_test\n",
    "            }\n",
    "\n",
    "            print(f\"  Test Accuracy: {test_accuracy:.3f}\")\n",
    "            print(f\"  AUC Score: {auc_score:.3f}\")\n",
    "            print(f\"  F1-Score: {f1:.3f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error training {name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return (pd.DataFrame(results).T, model_objects, predictions,\n",
    "            X_test, y_test, class_names, le, X_final, n_classes)\n",
    "\n",
    "def create_comprehensive_model_comparison_visualizations(results_df, model_objects, predictions,\n",
    "                                                       X_test, y_test, class_names, n_classes):\n",
    "    \"\"\"\n",
    "    Create comprehensive model comparison visualizations including ROC, PR curves,\n",
    "    AUC, Lorenz curves, and confusion matrices for ALL models\n",
    "    \"\"\"\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE MODEL COMPARISON VISUALIZATIONS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    if results_df.empty:\n",
    "        print(\"No results to visualize\")\n",
    "        return None\n",
    "\n",
    "    models = results_df.index\n",
    "    n_models = len(models)\n",
    "\n",
    "    # Create large figure for comprehensive comparison\n",
    "    fig = plt.figure(figsize=(20, 24))\n",
    "\n",
    "    # 1. ROC CURVES COMPARISON (All Models)\n",
    "    plt.subplot(4, 3, 1)\n",
    "\n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "\n",
    "    if n_classes == 2:\n",
    "        # Binary classification ROC\n",
    "        for i, model_name in enumerate(models):\n",
    "            if model_name in predictions:\n",
    "                y_pred_proba = predictions[model_name]['y_pred_proba']\n",
    "                fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                plt.plot(fpr, tpr, color=colors[i % len(colors)], lw=2,\n",
    "                        label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "    else:\n",
    "        # Multiclass ROC (macro-average)\n",
    "        for i, model_name in enumerate(models):\n",
    "            if model_name in predictions:\n",
    "                y_pred_proba = predictions[model_name]['y_pred_proba']\n",
    "\n",
    "                # Binarize the output\n",
    "                y_test_bin = label_binarize(y_test, classes=range(n_classes))\n",
    "\n",
    "                # Compute ROC curve and ROC area for each class\n",
    "                fpr = dict()\n",
    "                tpr = dict()\n",
    "                roc_auc = dict()\n",
    "                for j in range(n_classes):\n",
    "                    fpr[j], tpr[j], _ = roc_curve(y_test_bin[:, j], y_pred_proba[:, j])\n",
    "                    roc_auc[j] = auc(fpr[j], tpr[j])\n",
    "\n",
    "                # Compute macro-average ROC curve and ROC area\n",
    "                all_fpr = np.unique(np.concatenate([fpr[j] for j in range(n_classes)]))\n",
    "                mean_tpr = np.zeros_like(all_fpr)\n",
    "                for j in range(n_classes):\n",
    "                    mean_tpr += np.interp(all_fpr, fpr[j], tpr[j])\n",
    "                mean_tpr /= n_classes\n",
    "\n",
    "                macro_auc = auc(all_fpr, mean_tpr)\n",
    "                plt.plot(all_fpr, mean_tpr, color=colors[i % len(colors)], lw=2,\n",
    "                        label=f'{model_name} (Macro AUC = {macro_auc:.3f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves Comparison - All Models', fontweight='bold')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. PRECISION-RECALL CURVES COMPARISON (All Models)\n",
    "    plt.subplot(4, 3, 2)\n",
    "\n",
    "    for i, model_name in enumerate(models):\n",
    "        if model_name in predictions:\n",
    "            y_pred_proba = predictions[model_name]['y_pred_proba']\n",
    "\n",
    "            if n_classes == 2:\n",
    "                precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba[:, 1])\n",
    "                avg_precision = average_precision_score(y_test, y_pred_proba[:, 1])\n",
    "                plt.plot(recall_curve, precision_curve, color=colors[i % len(colors)], lw=2,\n",
    "                        label=f'{model_name} (AP = {avg_precision:.3f})')\n",
    "            else:\n",
    "                # For multiclass, compute average precision for each class and take macro average\n",
    "                y_test_bin = label_binarize(y_test, classes=range(n_classes))\n",
    "                avg_precision = average_precision_score(y_test_bin, y_pred_proba, average='macro')\n",
    "\n",
    "                # Plot average precision-recall curve\n",
    "                precision_dict = dict()\n",
    "                recall_dict = dict()\n",
    "                for j in range(n_classes):\n",
    "                    precision_dict[j], recall_dict[j], _ = precision_recall_curve(\n",
    "                        y_test_bin[:, j], y_pred_proba[:, j])\n",
    "\n",
    "                # Compute macro-average\n",
    "                all_recall = np.unique(np.concatenate([recall_dict[j] for j in range(n_classes)]))\n",
    "                mean_precision = np.zeros_like(all_recall)\n",
    "                for j in range(n_classes):\n",
    "                    mean_precision += np.interp(all_recall, recall_dict[j][::-1], precision_dict[j][::-1])\n",
    "                mean_precision /= n_classes\n",
    "\n",
    "                plt.plot(all_recall, mean_precision, color=colors[i % len(colors)], lw=2,\n",
    "                        label=f'{model_name} (Macro AP = {avg_precision:.3f})')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curves - All Models', fontweight='bold')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. AUC SCORES COMPARISON BAR CHART\n",
    "    plt.subplot(4, 3, 3)\n",
    "    auc_scores = [results_df.loc[model, 'AUC_Score'] for model in models]\n",
    "    bars = plt.bar(models, auc_scores, color=colors[:len(models)], alpha=0.7)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, auc_scores):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    plt.ylabel('AUC Score')\n",
    "    plt.title('AUC Scores Comparison', fontweight='bold')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 4-6. CONFUSION MATRICES FOR ALL MODELS\n",
    "    for i, model_name in enumerate(models):\n",
    "        plt.subplot(4, 3, 4 + i)\n",
    "\n",
    "        if model_name in predictions:\n",
    "            y_pred = predictions[model_name]['y_pred']\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "            # Normalize confusion matrix\n",
    "            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "            sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                       xticklabels=class_names, yticklabels=class_names)\n",
    "            plt.title(f'Confusion Matrix - {model_name}', fontweight='bold')\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "\n",
    "    # 7. LORENZ CURVE (Cumulative Gains Chart)\n",
    "    plt.subplot(4, 3, 7)\n",
    "\n",
    "    for i, model_name in enumerate(models):\n",
    "        if model_name in predictions:\n",
    "            y_pred_proba = predictions[model_name]['y_pred_proba']\n",
    "\n",
    "            if n_classes == 2:\n",
    "                # For binary classification, use positive class probabilities\n",
    "                y_scores = y_pred_proba[:, 1]\n",
    "                y_true_binary = y_test\n",
    "            else:\n",
    "                # For multiclass, use max probability as score\n",
    "                y_scores = np.max(y_pred_proba, axis=1)\n",
    "                y_true_binary = (y_test == np.argmax(y_pred_proba, axis=1)).astype(int)\n",
    "\n",
    "            # Sort by predicted probability (descending)\n",
    "            sorted_indices = np.argsort(y_scores)[::-1]\n",
    "            y_true_sorted = y_true_binary[sorted_indices]\n",
    "\n",
    "            # Calculate cumulative gains\n",
    "            cumulative_gains = np.cumsum(y_true_sorted) / np.sum(y_true_sorted)\n",
    "            population_percentage = np.arange(1, len(y_true_sorted) + 1) / len(y_true_sorted)\n",
    "\n",
    "            plt.plot(population_percentage, cumulative_gains, color=colors[i % len(colors)],\n",
    "                    lw=2, label=f'{model_name}')\n",
    "\n",
    "    # Add diagonal line (random classifier)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=1, label='Random')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('Population Percentage')\n",
    "    plt.ylabel('Cumulative Gains')\n",
    "    plt.title('Lorenz Curve (Cumulative Gains)', fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 8. MODEL PERFORMANCE METRICS COMPARISON\n",
    "    plt.subplot(4, 3, 8)\n",
    "\n",
    "    metrics = ['Test_Accuracy', 'Precision', 'Recall', 'F1_Score']\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.25\n",
    "\n",
    "    for i, model_name in enumerate(models):\n",
    "        values = [results_df.loc[model_name, metric] for metric in metrics]\n",
    "        plt.bar(x + i*width, values, width, label=model_name,\n",
    "                color=colors[i % len(colors)], alpha=0.7)\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Performance Metrics Comparison', fontweight='bold')\n",
    "    plt.xticks(x + width, metrics, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 9. OVERFITTING ANALYSIS\n",
    "    plt.subplot(4, 3, 9)\n",
    "\n",
    "    train_acc = [results_df.loc[model, 'Train_Accuracy'] for model in models]\n",
    "    test_acc = [results_df.loc[model, 'Test_Accuracy'] for model in models]\n",
    "\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.bar(x - width/2, train_acc, width, label='Train Accuracy', alpha=0.8)\n",
    "    plt.bar(x + width/2, test_acc, width, label='Test Accuracy', alpha=0.8)\n",
    "\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Overfitting Analysis', fontweight='bold')\n",
    "    plt.xticks(x, models, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 10. FEATURE IMPORTANCE (for tree-based models)\n",
    "    plt.subplot(4, 3, 10)\n",
    "\n",
    "    # Get feature importance from best performing model\n",
    "    best_model_name = results_df['Test_Accuracy'].idxmax()\n",
    "    if best_model_name in model_objects:\n",
    "        model_obj = model_objects[best_model_name]\n",
    "        if hasattr(model_obj, 'feature_importances_'):\n",
    "            # Use actual column names from X_final\n",
    "            if hasattr(model_obj, 'feature_importances_') and hasattr(X_test, 'columns'):\n",
    "                feature_names = X_test.columns\n",
    "                importances = model_obj.feature_importances_\n",
    "                importance_df = pd.DataFrame({\n",
    "                    'feature': feature_names,\n",
    "                    'importance': importances\n",
    "                }).sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "\n",
    "            plt.barh(range(len(importance_df)), importance_df['importance'],\n",
    "                    color='forestgreen', alpha=0.7)\n",
    "            plt.yticks(range(len(importance_df)), importance_df['feature'])\n",
    "            plt.xlabel('Importance')\n",
    "            plt.title(f'Top 10 Features - {best_model_name}', fontweight='bold')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 11. MODEL RANKING\n",
    "    plt.subplot(4, 3, 11)\n",
    "\n",
    "    # Calculate overall score\n",
    "    overall_score = (results_df['Test_Accuracy'] * 0.4 +\n",
    "                    results_df['F1_Score'] * 0.3 +\n",
    "                    results_df['AUC_Score'] * 0.3)\n",
    "\n",
    "    model_ranking = pd.DataFrame({\n",
    "        'Model': models,\n",
    "        'Overall_Score': overall_score\n",
    "    }).sort_values('Overall_Score', ascending=False)\n",
    "\n",
    "    bars = plt.barh(model_ranking['Model'], model_ranking['Overall_Score'],\n",
    "                   color='lightgreen', alpha=0.7)\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, model_ranking['Overall_Score']):\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{score:.3f}', ha='left', va='center', fontweight='bold')\n",
    "\n",
    "    plt.xlabel('Overall Score')\n",
    "    plt.title('Model Ranking (Weighted Score)', fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 12. PERFORMANCE SUMMARY TABLE\n",
    "    plt.subplot(4, 3, 12)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Create summary table\n",
    "    summary_data = []\n",
    "    for model in models:\n",
    "        row = results_df.loc[model]\n",
    "        summary_data.append([\n",
    "            model,\n",
    "            f\"{row['Test_Accuracy']:.3f}\",\n",
    "            f\"{row['F1_Score']:.3f}\",\n",
    "            f\"{row['AUC_Score']:.3f}\",\n",
    "            f\"{row['Overfitting_Gap']:.3f}\"\n",
    "        ])\n",
    "\n",
    "    table = plt.table(cellText=summary_data,\n",
    "                     colLabels=['Model', 'Accuracy', 'F1', 'AUC', 'Overfit'],\n",
    "                     cellLoc='center',\n",
    "                     loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.5)\n",
    "    plt.title('Performance Summary Table', fontweight='bold', pad=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model_ranking\n",
    "\n",
    "def generate_detailed_model_comparison_report(results_df, model_objects, predictions,\n",
    "                                            y_test, class_names, X_final):\n",
    "    \"\"\"Generate detailed comparison report for professor evaluation\"\"\"\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED MODEL COMPARISON REPORT\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Overall performance ranking\n",
    "    overall_score = (results_df['Test_Accuracy'] * 0.4 +\n",
    "                    results_df['F1_Score'] * 0.3 +\n",
    "                    results_df['AUC_Score'] * 0.3)\n",
    "    results_df['Overall_Score'] = overall_score\n",
    "    ranking = results_df.sort_values('Overall_Score', ascending=False)\n",
    "\n",
    "    print(f\"\\n MODEL PERFORMANCE RANKING:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, (model, row) in enumerate(ranking.iterrows(), 1):\n",
    "        print(f\"{i}. {model}\")\n",
    "        print(f\"   Overall Score: {row['Overall_Score']:.3f}\")\n",
    "        print(f\"   Test Accuracy: {row['Test_Accuracy']:.3f}\")\n",
    "        print(f\"   AUC Score: {row['AUC_Score']:.3f}\")\n",
    "        print(f\"   F1-Score: {row['F1_Score']:.3f}\")\n",
    "        print(f\"   Overfitting Gap: {row['Overfitting_Gap']:.3f}\")\n",
    "        print()\n",
    "\n",
    "    # Best model analysis\n",
    "    best_model = ranking.index[0]\n",
    "    print(f\" BEST PERFORMING MODEL: {best_model}\")\n",
    "    print(\"-\" * 50)\n",
    "    best_results = ranking.loc[best_model]\n",
    "    print(f\"- Achieves highest overall score: {best_results['Overall_Score']:.3f}\")\n",
    "    print(f\"- Test accuracy: {best_results['Test_Accuracy']:.3f}\")\n",
    "    print(f\"- Cross-validation accuracy: {best_results['CV_Accuracy_Mean']:.3f} ± {best_results['CV_Accuracy_Std']:.3f}\")\n",
    "    print(f\"- AUC score: {best_results['AUC_Score']:.3f}\")\n",
    "    print(f\"- F1-score: {best_results['F1_Score']:.3f}\")\n",
    "    print(f\"- Training time: {best_results['Training_Time']:.2f} seconds\")\n",
    "\n",
    "    # Model-specific insights\n",
    "    print(f\"\\n MODEL-SPECIFIC INSIGHTS:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for model in results_df.index:\n",
    "        row = results_df.loc[model]\n",
    "        print(f\"\\n{model}:\")\n",
    "        if row['Overfitting_Gap'] < 0.05:\n",
    "            print(f\"  Excellent generalization (gap: {row['Overfitting_Gap']:.3f})\")\n",
    "        elif row['Overfitting_Gap'] < 0.1:\n",
    "            print(f\"  Moderate overfitting (gap: {row['Overfitting_Gap']:.3f})\")\n",
    "        else:\n",
    "            print(f\"  High overfitting detected (gap: {row['Overfitting_Gap']:.3f})\")\n",
    "\n",
    "        if row['AUC_Score'] > 0.8:\n",
    "            print(f\"  Excellent discrimination ability (AUC: {row['AUC_Score']:.3f})\")\n",
    "        elif row['AUC_Score'] > 0.7:\n",
    "            print(f\"  Good discrimination ability (AUC: {row['AUC_Score']:.3f})\")\n",
    "        else:\n",
    "            print(f\"  Poor discrimination ability (AUC: {row['AUC_Score']:.3f})\")\n",
    "\n",
    "    # Detailed confusion matrix analysis for best model\n",
    "    if best_model in predictions:\n",
    "        y_pred_best = predictions[best_model]['y_pred']\n",
    "        cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "        print(f\"\\n DETAILED CONFUSION MATRIX ANALYSIS - {best_model}:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(pd.DataFrame(cm, index=class_names, columns=class_names))\n",
    "\n",
    "        # Per-class performance\n",
    "        print(f\"\\nPer-class Performance:\")\n",
    "        report = classification_report(y_test, y_pred_best, target_names=class_names, output_dict=True)\n",
    "        for class_name in class_names:\n",
    "            metrics = report[class_name]\n",
    "            print(f\"  {class_name}:\")\n",
    "            print(f\"    Precision: {metrics['precision']:.3f}\")\n",
    "            print(f\"    Recall: {metrics['recall']:.3f}\")\n",
    "            print(f\"    F1-Score: {metrics['f1-score']:.3f}\")\n",
    "\n",
    "    print(f\"\\n COMPLETE COMPARISON TABLE:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(results_df.round(3))\n",
    "\n",
    "    # Recommendations\n",
    "    print(f\"\\n RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"1. Deploy {best_model} for production use\")\n",
    "    print(f\"2. Monitor performance on new data to detect drift\")\n",
    "    print(f\"3. Consider ensemble methods combining top 2-3 models\")\n",
    "    print(f\"4. Focus on improving {ranking.index[-1]} model's performance\")\n",
    "\n",
    "    return ranking\n",
    "\n",
    "def main_classification_analysis(file_path):\n",
    "    \"\"\"Execute comprehensive classification analysis with enhanced model comparison\"\"\"\n",
    "    print(\"=\"*100)\n",
    "    print(\"COMPREHENSIVE CLASSIFICATION ANALYSIS - ENHANCED MODEL COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    # Load data\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8', low_memory=False)\n",
    "        print(f\" Loaded dataset: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='latin-1', low_memory=False)\n",
    "            print(f\" Loaded dataset with latin-1 encoding: {df.shape}\")\n",
    "        except Exception as e2:\n",
    "            print(f\" Could not load data: {e2}\")\n",
    "            return None\n",
    "\n",
    "    # Run classification modeling\n",
    "    try:\n",
    "        (results_df, model_objects, predictions, X_test, y_test,\n",
    "         class_names, le, X_final, n_classes) = classification_modeling_pipeline(df)\n",
    "\n",
    "        if results_df is not None and not results_df.empty:\n",
    "            # Create comprehensive visualizations\n",
    "            model_ranking = create_comprehensive_model_comparison_visualizations(\n",
    "                results_df, model_objects, predictions, X_test, y_test,\n",
    "                class_names, n_classes\n",
    "            )\n",
    "\n",
    "            # Generate detailed comparison report\n",
    "            detailed_ranking = generate_detailed_model_comparison_report(\n",
    "                results_df, model_objects, predictions, y_test, class_names, X_final\n",
    "            )\n",
    "\n",
    "            print(f\"\\n ENHANCED CLASSIFICATION ANALYSIS COMPLETED!\")\n",
    "            print(f\"\\nKey Enhancements:\")\n",
    "            print(f\"- ROC curves for all models with AUC comparison\")\n",
    "            print(f\"- Precision-Recall curves for all models\")\n",
    "            print(f\"- Individual confusion matrices for each model\")\n",
    "            print(f\"- Lorenz curves (cumulative gains) for model comparison\")\n",
    "            print(f\"- Comprehensive AUC score analysis\")\n",
    "            print(f\"- Detailed overfitting analysis\")\n",
    "            print(f\"- Model ranking with weighted scoring\")\n",
    "            print(f\"- Professor-friendly visualizations and reports\")\n",
    "\n",
    "        else:\n",
    "            print(\" No successful classification results\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error in classification pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'results': results_df,\n",
    "        'model_objects': model_objects,\n",
    "        'predictions': predictions,\n",
    "        'feature_matrix': X_final,\n",
    "        'class_names': class_names,\n",
    "        'label_encoder': le,\n",
    "        'model_ranking': model_ranking,\n",
    "        'detailed_ranking': detailed_ranking\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"Vehicles.csv\"  # Replace with your file path\n",
    "    np.random.seed(42)\n",
    "\n",
    "    print(\" Starting Enhanced Classification Analysis with Comprehensive Model Comparison...\")\n",
    "\n",
    "    # Run complete analysis\n",
    "    analysis_results = main_classification_analysis(file_path)\n",
    "\n",
    "    if analysis_results:\n",
    "        print(f\"\\n ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "        print(f\"\\nProfessor Evaluation Ready:\")\n",
    "        print(f\"✅ All models compared using ROC, PR curves, AUC, Lorenz curves\")\n",
    "        print(f\"✅ Individual confusion matrices for each model\")\n",
    "        print(f\"✅ Clear performance ranking and recommendations\")\n",
    "        print(f\"✅ Comprehensive visualizations for easy interpretation\")\n",
    "        print(f\"✅ Detailed analysis reports generated\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db2da3b-2652-40c8-9093-60bc6fe3743b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
